{
    "epoch": 10.0,
    "predict_exact_match": 31.4792,
    "predict_exact_match_for_ACE 2004": 15.5172,
    "predict_exact_match_for_ACE 2005_sample_15000": 22.1698,
    "predict_exact_match_for_ACE05": 10.7509,
    "predict_exact_match_for_ADE_corpus_sample_15000": 26.4019,
    "predict_exact_match_for_AnatEM": 38.5901,
    "predict_exact_match_for_Broad Tweet Corpus": 22.5,
    "predict_exact_match_for_CASIE": 4.0333,
    "predict_exact_match_for_CoNLL 2003_sample_15000": 35.795,
    "predict_exact_match_for_EEA": 3.2959,
    "predict_exact_match_for_EET": 17.6023,
    "predict_exact_match_for_FabNER": 3.1492,
    "predict_exact_match_for_FindVehicle": 17.914,
    "predict_exact_match_for_GENIA_NER": 9.3312,
    "predict_exact_match_for_GIDS": 15.3935,
    "predict_exact_match_for_HarveyNER": 73.5994,
    "predict_exact_match_for_MultiNERD": 35.42,
    "predict_exact_match_for_NER": 34.0658,
    "predict_exact_match_for_NYT11_sample_30000": 24.1192,
    "predict_exact_match_for_New-York-Times-RE": 39.82,
    "predict_exact_match_for_Ontonotes_sample_30000": 54.8293,
    "predict_exact_match_for_PHEE": 20.2996,
    "predict_exact_match_for_PolyglotNER": 33.57,
    "predict_exact_match_for_RE": 18.6683,
    "predict_exact_match_for_SciERC_sample_10000": 0.5038,
    "predict_exact_match_for_TweetNER7_sample_15000": 1.0417,
    "predict_exact_match_for_WikiANN en": 23.82,
    "predict_exact_match_for_WikiNeural": 38.1047,
    "predict_exact_match_for_bc2gm": 38.9,
    "predict_exact_match_for_bc4chemd": 48.3121,
    "predict_exact_match_for_bc5cdr": 17.2399,
    "predict_exact_match_for_conll04_sample_5000": 23.2639,
    "predict_exact_match_for_kbp37": 3.906,
    "predict_exact_match_for_mit-movie": 29.7297,
    "predict_exact_match_for_mit-restaurant": 31.3816,
    "predict_exact_match_for_ncbi": 46.1702,
    "predict_exact_match_for_semval-RE": 3.6437,
    "predict_gen_len": 15.4165,
    "predict_global_step": 0,
    "predict_loss": 0.8959367275238037,
    "predict_rouge1": 65.8072,
    "predict_rouge1_for_ACE 2004": 60.8511,
    "predict_rouge1_for_ACE 2005_sample_15000": 59.8618,
    "predict_rouge1_for_ACE05": 30.1645,
    "predict_rouge1_for_ADE_corpus_sample_15000": 74.0112,
    "predict_rouge1_for_AnatEM": 64.9251,
    "predict_rouge1_for_Broad Tweet Corpus": 51.262,
    "predict_rouge1_for_CASIE": 38.5389,
    "predict_rouge1_for_CoNLL 2003_sample_15000": 68.0481,
    "predict_rouge1_for_EEA": 41.6085,
    "predict_rouge1_for_EET": 53.6261,
    "predict_rouge1_for_FabNER": 61.2005,
    "predict_rouge1_for_FindVehicle": 83.076,
    "predict_rouge1_for_GENIA_NER": 54.7571,
    "predict_rouge1_for_GIDS": 70.687,
    "predict_rouge1_for_HarveyNER": 81.3843,
    "predict_rouge1_for_MultiNERD": 68.3979,
    "predict_rouge1_for_NER": 67.4512,
    "predict_rouge1_for_NYT11_sample_30000": 61.3407,
    "predict_rouge1_for_New-York-Times-RE": 76.5304,
    "predict_rouge1_for_Ontonotes_sample_30000": 78.3532,
    "predict_rouge1_for_PHEE": 66.9678,
    "predict_rouge1_for_PolyglotNER": 53.1563,
    "predict_rouge1_for_RE": 59.2402,
    "predict_rouge1_for_SciERC_sample_10000": 39.1682,
    "predict_rouge1_for_TweetNER7_sample_15000": 41.7804,
    "predict_rouge1_for_WikiANN en": 66.4908,
    "predict_rouge1_for_WikiNeural": 62.6898,
    "predict_rouge1_for_bc2gm": 62.4169,
    "predict_rouge1_for_bc4chemd": 64.2058,
    "predict_rouge1_for_bc5cdr": 56.4186,
    "predict_rouge1_for_conll04_sample_5000": 65.5263,
    "predict_rouge1_for_kbp37": 42.2713,
    "predict_rouge1_for_mit-movie": 75.4647,
    "predict_rouge1_for_mit-restaurant": 75.5901,
    "predict_rouge1_for_ncbi": 69.3412,
    "predict_rouge1_for_semval-RE": 30.196,
    "predict_rougeL": 64.7774,
    "predict_rougeL_for_ACE 2004": 57.5767,
    "predict_rougeL_for_ACE 2005_sample_15000": 57.2224,
    "predict_rougeL_for_ACE05": 29.2937,
    "predict_rougeL_for_ADE_corpus_sample_15000": 72.7781,
    "predict_rougeL_for_AnatEM": 64.3437,
    "predict_rougeL_for_Broad Tweet Corpus": 50.5099,
    "predict_rougeL_for_CASIE": 36.8848,
    "predict_rougeL_for_CoNLL 2003_sample_15000": 64.7538,
    "predict_rougeL_for_EEA": 38.552,
    "predict_rougeL_for_EET": 53.5335,
    "predict_rougeL_for_FabNER": 56.8455,
    "predict_rougeL_for_FindVehicle": 82.5624,
    "predict_rougeL_for_GENIA_NER": 49.236,
    "predict_rougeL_for_GIDS": 69.9193,
    "predict_rougeL_for_HarveyNER": 81.2024,
    "predict_rougeL_for_MultiNERD": 67.7882,
    "predict_rougeL_for_NER": 66.5559,
    "predict_rougeL_for_NYT11_sample_30000": 60.0123,
    "predict_rougeL_for_New-York-Times-RE": 72.0099,
    "predict_rougeL_for_Ontonotes_sample_30000": 77.4122,
    "predict_rougeL_for_PHEE": 65.3036,
    "predict_rougeL_for_PolyglotNER": 52.5315,
    "predict_rougeL_for_RE": 57.3649,
    "predict_rougeL_for_SciERC_sample_10000": 35.6873,
    "predict_rougeL_for_TweetNER7_sample_15000": 39.491,
    "predict_rougeL_for_WikiANN en": 65.8024,
    "predict_rougeL_for_WikiNeural": 62.1378,
    "predict_rougeL_for_bc2gm": 61.4124,
    "predict_rougeL_for_bc4chemd": 63.8167,
    "predict_rougeL_for_bc5cdr": 54.5142,
    "predict_rougeL_for_conll04_sample_5000": 64.2529,
    "predict_rougeL_for_kbp37": 41.4725,
    "predict_rougeL_for_mit-movie": 74.1649,
    "predict_rougeL_for_mit-restaurant": 74.5291,
    "predict_rougeL_for_ncbi": 68.7486,
    "predict_rougeL_for_semval-RE": 30.0795,
    "predict_runtime": 2171.3419,
    "predict_samples": 151084,
    "predict_samples_per_second": 69.581,
    "predict_steps_per_second": 1.087,
    "train_loss": 1.729793385442036,
    "train_runtime": 41291.3051,
    "train_samples": 525719,
    "train_samples_per_second": 127.32,
    "train_steps_per_second": 0.497
}